<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/xmsyZZxTSiGwH0Zq+dDubnLIfsg</id>
  <title>arXiv Query: search_query=all:AI&amp;id_list=&amp;start=0&amp;max_results=25</title>
  <updated>2026-02-20T17:55:48Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:AI&amp;start=0&amp;max_results=25&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>25</opensearch:itemsPerPage>
  <opensearch:totalResults>51260</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2501.02842v1</id>
    <title>Foundations of GenIR</title>
    <updated>2025-01-06T08:38:29Z</updated>
    <link href="https://arxiv.org/abs/2501.02842v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2501.02842v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The chapter discusses the foundational impact of modern generative AI models on information access (IA) systems. In contrast to traditional AI, the large-scale training and superior data modeling of generative AI models enable them to produce high-quality, human-like responses, which brings brand new opportunities for the development of IA paradigms. In this chapter, we identify and introduce two of them in details, i.e., information generation and information synthesis. Information generation allows AI to create tailored content addressing user needs directly, enhancing user experience with immediate, relevant outputs. Information synthesis leverages the ability of generative AI to integrate and reorganize existing information, providing grounded responses and mitigating issues like model hallucination, which is particularly valuable in scenarios requiring precision and external knowledge. This chapter delves into the foundational aspects of generative models, including architecture, scaling, and training, and discusses their applications in multi-modal scenarios. Additionally, it examines the retrieval-augmented generation paradigm and other methods for corpus modeling and understanding, demonstrating how generative AI can enhance information access systems. It also summarizes potential challenges and fruitful directions for future studies.</summary>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-01-06T08:38:29Z</published>
    <arxiv:comment>Chapter 2 of the book on Information Access in the Era of Generative AI</arxiv:comment>
    <arxiv:primary_category term="cs.IR"/>
    <author>
      <name>Qingyao Ai</name>
    </author>
    <author>
      <name>Jingtao Zhan</name>
    </author>
    <author>
      <name>Yiqun Liu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.16513v1</id>
    <title>Competing Visions of Ethical AI: A Case Study of OpenAI</title>
    <updated>2026-01-23T07:26:45Z</updated>
    <link href="https://arxiv.org/abs/2601.16513v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.16513v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Introduction. AI Ethics is framed distinctly across actors and stakeholder groups. We report results from a case study of OpenAI analysing ethical AI discourse. Method. Research addressed: How has OpenAI's public discourse leveraged 'ethics', 'safety', 'alignment' and adjacent related concepts over time, and what does discourse signal about framing in practice? A structured corpus, differentiating between communication for a general audience and communication with an academic audience, was assembled from public documentation. Analysis. Qualitative content analysis of ethical themes combined inductively derived and deductively applied codes. Quantitative analysis leveraged computational content analysis methods via NLP to model topics and quantify changes in rhetoric over time. Visualizations report aggregate results. For reproducible results, we have released our code at https://github.com/famous-blue-raincoat/AI_Ethics_Discourse. Results. Results indicate that safety and risk discourse dominate OpenAI's public communication and documentation, without applying academic and advocacy ethics frameworks or vocabularies. Conclusions. Implications for governance are presented, along with discussion of ethics-washing practices in industry.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-23T07:26:45Z</published>
    <arxiv:comment>iConference 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <author>
      <name>Melissa Wilfley</name>
    </author>
    <author>
      <name>Mengting Ai</name>
    </author>
    <author>
      <name>Madelyn Rose Sanfilippo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.12400v1</id>
    <title>Towards The Ultimate Brain: Exploring Scientific Discovery with ChatGPT AI</title>
    <updated>2023-07-08T09:59:22Z</updated>
    <link href="https://arxiv.org/abs/2308.12400v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2308.12400v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper presents a novel approach to scientific discovery using an artificial intelligence (AI) environment known as ChatGPT, developed by OpenAI. This is the first paper entirely generated with outputs from ChatGPT. We demonstrate how ChatGPT can be instructed through a gamification environment to define and benchmark hypothetical physical theories. Through this environment, ChatGPT successfully simulates the creation of a new improved model, called GPT$^4$, which combines the concepts of GPT in AI (generative pretrained transformer) and GPT in physics (generalized probabilistic theory). We show that GPT$^4$ can use its built-in mathematical and statistical capabilities to simulate and analyze physical laws and phenomena. As a demonstration of its language capabilities, GPT$^4$ also generates a limerick about itself. Overall, our results demonstrate the promising potential for human-AI collaboration in scientific discovery, as well as the importance of designing systems that effectively integrate AI's capabilities with human intelligence.</summary>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-07-08T09:59:22Z</published>
    <arxiv:comment>26 pages. To appear in AI Magazine</arxiv:comment>
    <arxiv:primary_category term="cs.OH"/>
    <arxiv:journal_ref>AI Magazine 44, 328 (2023)</arxiv:journal_ref>
    <author>
      <name>Gerardo Adesso</name>
    </author>
    <arxiv:doi>10.1002/aaai.12113</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1002/aaai.12113" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01298v2</id>
    <title>Meaningful human control: actionable properties for AI system development</title>
    <updated>2022-05-19T15:28:01Z</updated>
    <link href="https://arxiv.org/abs/2112.01298v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2112.01298v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>How can humans remain in control of artificial intelligence (AI)-based systems designed to perform tasks autonomously? Such systems are increasingly ubiquitous, creating benefits - but also undesirable situations where moral responsibility for their actions cannot be properly attributed to any particular person or group. The concept of meaningful human control has been proposed to address responsibility gaps and mitigate them by establishing conditions that enable a proper attribution of responsibility for humans; however, clear requirements for researchers, designers, and engineers are yet inexistent, making the development of AI-based systems that remain under meaningful human control challenging. In this paper, we address the gap between philosophical theory and engineering practice by identifying, through an iterative process of abductive thinking, four actionable properties for AI-based systems under meaningful human control, which we discuss making use of two applications scenarios: automated vehicles and AI-based hiring. First, a system in which humans and AI algorithms interact should have an explicitly defined domain of morally loaded situations within which the system ought to operate. Second, humans and AI agents within the system should have appropriate and mutually compatible representations. Third, responsibility attributed to a human should be commensurate with that human's ability and authority to control the system. Fourth, there should be explicit links between the actions of the AI agents and actions of humans who are aware of their moral responsibility. We argue that these four properties will support practically-minded professionals to take concrete steps toward designing and engineering for AI systems that facilitate meaningful human control.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-11-25T11:05:37Z</published>
    <arxiv:comment>Preprint. Published AI and Ethics (2022): https://doi.org/10.1007/s43681-022-00167-3</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <arxiv:journal_ref>AI Ethics (2022)</arxiv:journal_ref>
    <author>
      <name>Luciano Cavalcante Siebert</name>
    </author>
    <author>
      <name>Maria Luce Lupetti</name>
    </author>
    <author>
      <name>Evgeni Aizenberg</name>
    </author>
    <author>
      <name>Niek Beckers</name>
    </author>
    <author>
      <name>Arkady Zgonnikov</name>
    </author>
    <author>
      <name>Herman Veluwenkamp</name>
    </author>
    <author>
      <name>David Abbink</name>
    </author>
    <author>
      <name>Elisa Giaccardi</name>
    </author>
    <author>
      <name>Geert-Jan Houben</name>
    </author>
    <author>
      <name>Catholijn M. Jonker</name>
    </author>
    <author>
      <name>Jeroen van den Hoven</name>
    </author>
    <author>
      <name>Deborah Forster</name>
    </author>
    <author>
      <name>Reginald L. Lagendijk</name>
    </author>
    <arxiv:doi>10.1007/s43681-022-00167-3</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1007/s43681-022-00167-3" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.00025v3</id>
    <title>Need of AI in Modern Education: in the Eyes of Explainable AI (xAI)</title>
    <updated>2025-01-02T21:59:22Z</updated>
    <link href="https://arxiv.org/abs/2408.00025v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2408.00025v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Modern Education is not \textit{Modern} without AI. However, AI's complex nature makes understanding and fixing problems challenging. Research worldwide shows that a parent's income greatly influences a child's education. This led us to explore how AI, especially complex models, makes important decisions using Explainable AI tools. Our research uncovered many complexities linked to parental income and offered reasonable explanations for these decisions. However, we also found biases in AI that go against what we want from AI in education: clear transparency and equal access for everyone. These biases can impact families and children's schooling, highlighting the need for better AI solutions that offer fair opportunities to all. This chapter tries to shed light on the complex ways AI operates, especially concerning biases. These are the foundational steps towards better educational policies, which include using AI in ways that are more reliable, accountable, and beneficial for everyone involved.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-07-31T08:11:33Z</published>
    <arxiv:comment>Chapter in the book: Blockchain and AI in Shaping the Modern Education System, CRC Press, Taylor &amp; Francis Group, USA</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <arxiv:journal_ref>Blockchain and AI in Shaping the Modern Education System 2025</arxiv:journal_ref>
    <author>
      <name>Supriya Manna</name>
    </author>
    <author>
      <name>Niladri Sett</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.15284v6</id>
    <title>Beyond principlism: Practical strategies for ethical AI use in research practices</title>
    <updated>2025-06-20T02:59:45Z</updated>
    <link href="https://arxiv.org/abs/2401.15284v6" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2401.15284v6" rel="related" type="application/pdf" title="pdf"/>
    <summary>The rapid adoption of generative artificial intelligence (AI) in scientific research, particularly large language models (LLMs), has outpaced the development of ethical guidelines, leading to a "Triple-Too" problem: too many high-level ethical initiatives, too abstract principles lacking contextual and practical relevance, and too much focus on restrictions and risks over benefits and utilities. Existing approaches--principlism (reliance on abstract ethical principles), formalism (rigid application of rules), and technological solutionism (overemphasis on technological fixes)--offer little practical guidance for addressing ethical challenges of AI in scientific research practices. To bridge the gap between abstract principles and day-to-day research practices, a user-centered, realism-inspired approach is proposed here. It outlines five specific goals for ethical AI use: 1) understanding model training and output, including bias mitigation strategies; 2) respecting privacy, confidentiality, and copyright; 3) avoiding plagiarism and policy violations; 4) applying AI beneficially compared to alternatives; and 5) using AI transparently and reproducibly. Each goal is accompanied by actionable strategies and realistic cases of misuse and corrective measures. I argue that ethical AI application requires evaluating its utility against existing alternatives rather than isolated performance metrics. Additionally, I propose documentation guidelines to enhance transparency and reproducibility in AI-assisted research. Moving forward, we need targeted professional development, training programs, and balanced enforcement mechanisms to promote responsible AI use while fostering innovation. By refining these ethical guidelines and adapting them to emerging AI capabilities, we can accelerate scientific progress without compromising research integrity.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-01-27T03:53:25Z</published>
    <arxiv:comment>Published in: AI and Ethics, 2025</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <arxiv:journal_ref>AI and Ethics, 5, 2719-2731 (2025)</arxiv:journal_ref>
    <author>
      <name>Zhicheng Lin</name>
    </author>
    <arxiv:doi>10.1007/s43681-024-00585-5</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1007/s43681-024-00585-5" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.16770v1</id>
    <title>DeBiasMe: De-biasing Human-AI Interactions with Metacognitive AIED (AI in Education) Interventions</title>
    <updated>2025-04-23T14:41:31Z</updated>
    <link href="https://arxiv.org/abs/2504.16770v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2504.16770v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>While generative artificial intelligence (Gen AI) increasingly transforms academic environments, a critical gap exists in understanding and mitigating human biases in AI interactions, such as anchoring and confirmation bias. This position paper advocates for metacognitive AI literacy interventions to help university students critically engage with AI and address biases across the Human-AI interaction workflows. The paper presents the importance of considering (1) metacognitive support with deliberate friction focusing on human bias; (2) bi-directional Human-AI interaction intervention addressing both input formulation and output interpretation; and (3) adaptive scaffolding that responds to diverse user engagement patterns. These frameworks are illustrated through ongoing work on "DeBiasMe," AIED (AI in Education) interventions designed to enhance awareness of cognitive biases while empowering user agency in AI interactions. The paper invites multiple stakeholders to engage in discussions on design and evaluation methods for scaffolding mechanisms, bias visualization, and analysis frameworks. This position contributes to the emerging field of AI-augmented learning by emphasizing the critical role of metacognition in helping students navigate the complex interaction between human, statistical, and systemic biases in AI use while highlighting how cognitive adaptation to AI systems must be explicitly integrated into comprehensive AI literacy frameworks.</summary>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-04-23T14:41:31Z</published>
    <arxiv:comment>Presented at the 2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING</arxiv:comment>
    <arxiv:primary_category term="cs.HC"/>
    <arxiv:journal_ref>Proceedings of the 2025 ACM CHI Workshop on Human-AI Interaction for Augmented Reasoning</arxiv:journal_ref>
    <author>
      <name>Chaeyeon Lim</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.12434v1</id>
    <title>Expansive Participatory AI: Supporting Dreaming within Inequitable Institutions</title>
    <updated>2022-11-22T17:44:03Z</updated>
    <link href="https://arxiv.org/abs/2211.12434v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2211.12434v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Participatory Artificial Intelligence (PAI) has recently gained interest by researchers as means to inform the design of technology through collective's lived experience. PAI has a greater promise than that of providing useful input to developers, it can contribute to the process of democratizing the design of technology, setting the focus on what should be designed. However, in the process of PAI there existing institutional power dynamics that hinder the realization of expansive dreams and aspirations of the relevant stakeholders. In this work we propose co-design principals for AI that address institutional power dynamics focusing on Participatory AI with youth.</summary>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-11-22T17:44:03Z</published>
    <arxiv:comment>3 pages, Human-Centered AI workshop</arxiv:comment>
    <arxiv:primary_category term="cs.HC"/>
    <arxiv:journal_ref>Human-Centered AI workshop (HCAI) 2022, NEURIPS</arxiv:journal_ref>
    <author>
      <name>Michael Alan Chang</name>
    </author>
    <author>
      <name>Shiran Dudy</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08817v2</id>
    <title>Exploring utilization of generative AI for research and education in data-driven materials science</title>
    <updated>2025-08-04T05:55:53Z</updated>
    <link href="https://arxiv.org/abs/2504.08817v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2504.08817v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Generative AI has recently had a profound impact on various fields, including daily life, research, and education. To explore its efficient utilization in data-driven materials science, we organized a hackathon -- AIMHack2024 -- in July 2024. In this hackathon, researchers from fields such as materials science, information science, bioinformatics, and condensed matter physics worked together to explore how generative AI can facilitate research and education. Based on the results of the hackathon, this paper presents topics related to (1) conducting AI-assisted software trials, (2) building AI tutors for software, and (3) developing GUI applications for software. While generative AI continues to evolve rapidly, this paper provides an early record of its application in data-driven materials science and highlights strategies for integrating AI into research and education.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ed-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-04-09T11:15:21Z</published>
    <arxiv:comment>13 pages, 3 figures</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <author>
      <name>Takahiro Misawa</name>
    </author>
    <author>
      <name>Ai Koizumi</name>
    </author>
    <author>
      <name>Ryo Tamura</name>
    </author>
    <author>
      <name>Kazuyoshi Yoshimi</name>
    </author>
    <arxiv:doi>10.1080/27660400.2025.2535956</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1080/27660400.2025.2535956" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.09138v1</id>
    <title>White-Box AI Model: Next Frontier of Wireless Communications</title>
    <updated>2025-04-12T08:57:17Z</updated>
    <link href="https://arxiv.org/abs/2504.09138v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2504.09138v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>White-box AI (WAI), or explainable AI (XAI) model, a novel tool to achieve the reasoning behind decisions and predictions made by the AI algorithms, makes it more understandable and transparent. It offers a new approach to address key challenges of interpretability and mathematical validation in traditional black-box models. In this paper, WAI-aided wireless communication systems are proposed and investigated thoroughly to utilize the promising capabilities. First, we introduce the fundamental principles of WAI. Then, a detailed comparison between WAI and traditional black-box model is conducted in terms of optimization objectives and architecture design, with a focus on deep neural networks (DNNs) and transformer networks. Furthermore, in contrast to the traditional black-box methods, WAI leverages theory-driven causal modeling and verifiable optimization paths, thereby demonstrating potential advantages in areas such as signal processing and resource allocation. Finally, we outline future research directions for the integration of WAI in wireless communication systems.</summary>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-04-12T08:57:17Z</published>
    <arxiv:primary_category term="cs.IT"/>
    <author>
      <name>Jiayao Yang</name>
    </author>
    <author>
      <name>Jiayi Zhang</name>
    </author>
    <author>
      <name>Bokai Xu</name>
    </author>
    <author>
      <name>Jiakang Zheng</name>
    </author>
    <author>
      <name>Zhilong Liu</name>
    </author>
    <author>
      <name>Ziheng Liu</name>
    </author>
    <author>
      <name>Dusit Niyato</name>
    </author>
    <author>
      <name>MÃ©rouane Debbah</name>
    </author>
    <author>
      <name>Zhu Han</name>
    </author>
    <author>
      <name>Bo Ai</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.18252v3</id>
    <title>Privacy and Copyright Protection in Generative AI: A Lifecycle Perspective</title>
    <updated>2024-11-17T12:09:49Z</updated>
    <link href="https://arxiv.org/abs/2311.18252v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2311.18252v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>The advent of Generative AI has marked a significant milestone in artificial intelligence, demonstrating remarkable capabilities in generating realistic images, texts, and data patterns. However, these advancements come with heightened concerns over data privacy and copyright infringement, primarily due to the reliance on vast datasets for model training. Traditional approaches like differential privacy, machine unlearning, and data poisoning only offer fragmented solutions to these complex issues. Our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle. We advocate for integrated approaches that combines technical innovation with ethical foresight, holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective. This work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in Generative AI.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-11-30T05:03:08Z</published>
    <arxiv:comment>Accepted by 2024 IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI (CAIN)</arxiv:comment>
    <arxiv:primary_category term="cs.SE"/>
    <arxiv:journal_ref>Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering-Software Engineering for AI. 2024. pp.92-97</arxiv:journal_ref>
    <author>
      <name>Dawen Zhang</name>
    </author>
    <author>
      <name>Boming Xia</name>
    </author>
    <author>
      <name>Yue Liu</name>
    </author>
    <author>
      <name>Xiwei Xu</name>
    </author>
    <author>
      <name>Thong Hoang</name>
    </author>
    <author>
      <name>Zhenchang Xing</name>
    </author>
    <author>
      <name>Mark Staples</name>
    </author>
    <author>
      <name>Qinghua Lu</name>
    </author>
    <author>
      <name>Liming Zhu</name>
    </author>
    <arxiv:doi>10.1145/3644815.3644952</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1145/3644815.3644952" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.11056v1</id>
    <title>BERT4beam: Large AI Model Enabled Generalized Beamforming Optimization</title>
    <updated>2025-09-14T02:49:29Z</updated>
    <link href="https://arxiv.org/abs/2509.11056v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2509.11056v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Artificial intelligence (AI) is anticipated to emerge as a pivotal enabler for the forthcoming sixth-generation (6G) wireless communication systems. However, current research efforts regarding large AI models for wireless communications primarily focus on fine-tuning pre-trained large language models (LLMs) for specific tasks. This paper investigates the large-scale AI model designed for beamforming optimization to adapt and generalize to diverse tasks defined by system utilities and scales. We propose a novel framework based on bidirectional encoder representations from transformers (BERT), termed BERT4beam. We aim to formulate the beamforming optimization problem as a token-level sequence learning task, perform tokenization of the channel state information, construct the BERT model, and conduct task-specific pre-training and fine-tuning strategies. Based on the framework, we propose two BERT-based approaches for single-task and multi-task beamforming optimization, respectively. Both approaches are generalizable for varying user scales. Moreover, the former can adapt to varying system utilities and antenna configurations by re-configuring the input and output module of the BERT model, while the latter, termed UBERT, can directly generalize to diverse tasks, due to a finer-grained tokenization strategy. Extensive simulation results demonstrate that the two proposed approaches can achieve near-optimal performance and outperform existing AI models across various beamforming optimization tasks, showcasing strong adaptability and generalizability.</summary>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-09-14T02:49:29Z</published>
    <arxiv:primary_category term="eess.SY"/>
    <author>
      <name>Yuhang Li</name>
    </author>
    <author>
      <name>Yang Lu</name>
    </author>
    <author>
      <name>Wei Chen</name>
    </author>
    <author>
      <name>Bo Ai</name>
    </author>
    <author>
      <name>Zhiguo Ding</name>
    </author>
    <author>
      <name>Dusit Niyato</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.14689v1</id>
    <title>Designing AI Systems that Augment Human Performed vs. Demonstrated Critical Thinking</title>
    <updated>2025-04-20T17:40:28Z</updated>
    <link href="https://arxiv.org/abs/2504.14689v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2504.14689v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The recent rapid advancement of LLM-based AI systems has accelerated our search and production of information. While the advantages brought by these systems seemingly improve the performance or efficiency of human activities, they do not necessarily enhance human capabilities. Recent research has started to examine the impact of generative AI on individuals' cognitive abilities, especially critical thinking. Based on definitions of critical thinking across psychology and education, this position paper proposes the distinction between demonstrated and performed critical thinking in the era of generative AI and discusses the implication of this distinction in research and development of AI systems that aim to augment human critical thinking.</summary>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-04-20T17:40:28Z</published>
    <arxiv:comment>Presented at the 2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning,</arxiv:comment>
    <arxiv:primary_category term="cs.HC"/>
    <arxiv:journal_ref>Proceedings of the 2025 ACM CHI Workshop on Human-AI Interaction for Augmented Reasoning</arxiv:journal_ref>
    <author>
      <name>Katelyn Xiaoying Mei</name>
    </author>
    <author>
      <name>Nic Weber</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.16110v1</id>
    <title>Expert-Guided LLM Reasoning for Battery Discovery: From AI-Driven Hypothesis to Synthesis and Characterization</title>
    <updated>2025-07-21T23:46:11Z</updated>
    <link href="https://arxiv.org/abs/2507.16110v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2507.16110v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large language models (LLMs) leverage chain-of-thought (CoT) techniques to tackle complex problems, representing a transformative breakthrough in artificial intelligence (AI). However, their reasoning capabilities have primarily been demonstrated in solving math and coding problems, leaving their potential for domain-specific applications-such as battery discovery-largely unexplored. Inspired by the idea that reasoning mirrors a form of guided search, we introduce ChatBattery, a novel agentic framework that integrates domain knowledge to steer LLMs toward more effective reasoning in materials design. Using ChatBattery, we successfully identify, synthesize, and characterize three novel lithium-ion battery cathode materials, which achieve practical capacity improvements of 28.8%, 25.2%, and 18.5%, respectively, over the widely used cathode material, LiNi0.8Mn0.1Co0.1O2 (NMC811). Beyond this discovery, ChatBattery paves a new path by showing a successful LLM-driven and reasoning-based platform for battery materials invention. This complete AI-driven cycle-from design to synthesis to characterization-demonstrates the transformative potential of AI-driven reasoning in revolutionizing materials discovery.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-07-21T23:46:11Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Shengchao Liu</name>
    </author>
    <author>
      <name>Hannan Xu</name>
    </author>
    <author>
      <name>Yan Ai</name>
    </author>
    <author>
      <name>Huanxin Li</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <author>
      <name>Harry Guo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.14538v4</id>
    <title>Overview of AI and Communication for 6G Network: Fundamentals, Challenges, and Future Research Opportunities</title>
    <updated>2025-02-13T07:40:46Z</updated>
    <link href="https://arxiv.org/abs/2412.14538v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2412.14538v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>With the growing demand for seamless connectivity and intelligent communication, the integration of artificial intelligence (AI) and sixth-generation (6G) communication networks has emerged as a transformative paradigm. By embedding AI capabilities across various network layers, this integration enables optimized resource allocation, improved efficiency, and enhanced system robust performance, particularly in intricate and dynamic environments. This paper presents a comprehensive overview of AI and communication for 6G networks, with a focus on emphasizing their foundational principles, inherent challenges, and future research opportunities. We first review the integration of AI and communications in the context of 6G, exploring the driving factors behind incorporating AI into wireless communications, as well as the vision for the convergence of AI and 6G. The discourse then transitions to a detailed exposition of the envisioned integration of AI within 6G networks, delineated across three progressive developmental stages. The first stage, AI for Network, focuses on employing AI to augment network performance, optimize efficiency, and enhance user service experiences. The second stage, Network for AI, highlights the role of the network in facilitating and buttressing AI operations and presents key enabling technologies, such as digital twins for AI and semantic communication. In the final stage, AI as a Service, it is anticipated that future 6G networks will innately provide AI functions as services, supporting application scenarios like immersive communication and intelligent industrial robots. In addition, we conduct an in-depth analysis of the critical challenges faced by the integration of AI and communications in 6G. Finally, we outline promising future research opportunities that are expected to drive the development and refinement of AI and 6G communications.</summary>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-12-19T05:36:34Z</published>
    <arxiv:primary_category term="cs.NI"/>
    <arxiv:journal_ref>Sci China Inf Sci, 2025, 68(7): 171301</arxiv:journal_ref>
    <author>
      <name>Qimei Cui</name>
    </author>
    <author>
      <name>Xiaohu You</name>
    </author>
    <author>
      <name>Ni Wei</name>
    </author>
    <author>
      <name>Guoshun Nan</name>
    </author>
    <author>
      <name>Xuefei Zhang</name>
    </author>
    <author>
      <name>Jianhua Zhang</name>
    </author>
    <author>
      <name>Xinchen Lyu</name>
    </author>
    <author>
      <name>Ming Ai</name>
    </author>
    <author>
      <name>Xiaofeng Tao</name>
    </author>
    <author>
      <name>Zhiyong Feng</name>
    </author>
    <author>
      <name>Ping Zhang</name>
    </author>
    <author>
      <name>Qingqing Wu</name>
    </author>
    <author>
      <name>Meixia Tao</name>
    </author>
    <author>
      <name>Yongming Huang</name>
    </author>
    <author>
      <name>Chongwen Huang</name>
    </author>
    <author>
      <name>Guangyi Liu</name>
    </author>
    <author>
      <name>Chenghui Peng</name>
    </author>
    <author>
      <name>Zhiwen Pan</name>
    </author>
    <author>
      <name>Tao Sun</name>
    </author>
    <author>
      <name>Dusit Niyato</name>
    </author>
    <author>
      <name>Tao Chen</name>
    </author>
    <author>
      <name>Muhammad Khurram Khan</name>
    </author>
    <author>
      <name>Abbas Jamalipour</name>
    </author>
    <author>
      <name>Mohsen Guizani</name>
    </author>
    <author>
      <name>Chau Yuen</name>
    </author>
    <arxiv:doi>10.1007/s11432-024-4337-1</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1007/s11432-024-4337-1" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.00183v2</id>
    <title>Lattica: A Decentralized Cross-NAT Communication Framework for Scalable AI Inference and Training</title>
    <updated>2025-10-02T19:22:39Z</updated>
    <link href="https://arxiv.org/abs/2510.00183v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2510.00183v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>The rapid expansion of distributed Artificial Intelligence (AI) workloads beyond centralized data centers creates a demand for new communication substrates. These substrates must operate reliably in heterogeneous and permissionless environments, where Network Address Translators (NATs) and firewalls impose significant constraints. Existing solutions, however, are either designed for controlled data center deployments or implemented as monolithic systems that tightly couple machine learning logic with networking code. To address these limitations, we present Lattica, a decentralized cross-NAT communication framework designed to support distributed AI systems. Lattica integrates three core components. First, it employs a robust suite of NAT traversal mechanisms to establish a globally addressable peer-to-peer mesh. Second, it provides a decentralized data store based on Conflict-free Replicated Data Types (CRDTs), ensuring verifiable and eventually consistent state replication. Third, it incorporates a content discovery layer that leverages distributed hash tables (DHTs) together with an optimized RPC protocol for efficient model synchronization. By integrating these components, Lattica delivers a complete protocol stack for sovereign, resilient, and scalable AI systems that operate independently of centralized intermediaries. It is directly applicable to edge intelligence, collaborative reinforcement learning, and other large-scale distributed machine learning scenarios.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-09-30T19:03:24Z</published>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Ween Yang</name>
    </author>
    <author>
      <name>Jason Liu</name>
    </author>
    <author>
      <name>Suli Wang</name>
    </author>
    <author>
      <name>Xinyuan Song</name>
    </author>
    <author>
      <name>Lynn Ai</name>
    </author>
    <author>
      <name>Eric Yang</name>
    </author>
    <author>
      <name>Bill Shi</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15894v1</id>
    <title>Supporting Data-Frame Dynamics in AI-assisted Decision Making</title>
    <updated>2025-04-22T13:36:06Z</updated>
    <link href="https://arxiv.org/abs/2504.15894v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2504.15894v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>High stakes decision-making often requires a continuous interplay between evolving evidence and shifting hypotheses, a dynamic that is not well supported by current AI decision support systems. In this paper, we introduce a mixed-initiative framework for AI assisted decision making that is grounded in the data-frame theory of sensemaking and the evaluative AI paradigm. Our approach enables both humans and AI to collaboratively construct, validate, and adapt hypotheses. We demonstrate our framework with an AI-assisted skin cancer diagnosis prototype that leverages a concept bottleneck model to facilitate interpretable interactions and dynamic updates to diagnostic hypotheses.</summary>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-04-22T13:36:06Z</published>
    <arxiv:comment>Presented at the 2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING</arxiv:comment>
    <arxiv:primary_category term="cs.HC"/>
    <arxiv:journal_ref>Proceedings of the 2025 ACM CHI Workshop on Human-AI Interaction for Augmented Reasoning</arxiv:journal_ref>
    <author>
      <name>Chengbo Zheng</name>
    </author>
    <author>
      <name>Tim Miller</name>
    </author>
    <author>
      <name>Alina Bialkowski</name>
    </author>
    <author>
      <name>H Peter Soyer</name>
    </author>
    <author>
      <name>Monika Janda</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.13042v1</id>
    <title>How Do AI Companies "Fine-Tune" Policy? Examining Regulatory Capture in AI Governance</title>
    <updated>2024-10-16T21:06:54Z</updated>
    <link href="https://arxiv.org/abs/2410.13042v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2410.13042v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Industry actors in the United States have gained extensive influence in conversations about the regulation of general-purpose artificial intelligence (AI) systems. Although industry participation is an important part of the policy process, it can also cause regulatory capture, whereby industry co-opts regulatory regimes to prioritize private over public welfare. Capture of AI policy by AI developers and deployers could hinder such regulatory goals as ensuring the safety, fairness, beneficence, transparency, or innovation of general-purpose AI systems. In this paper, we first introduce different models of regulatory capture from the social science literature. We then present results from interviews with 17 AI policy experts on what policy outcomes could compose regulatory capture in US AI policy, which AI industry actors are influencing the policy process, and whether and how AI industry actors attempt to achieve outcomes of regulatory capture. Experts were primarily concerned with capture leading to a lack of AI regulation, weak regulation, or regulation that over-emphasizes certain policy goals over others. Experts most commonly identified agenda-setting (15 of 17 interviews), advocacy (13), academic capture (10), information management (9), cultural capture through status (7), and media capture (7) as channels for industry influence. To mitigate these particular forms of industry influence, we recommend systemic changes in developing technical expertise in government and civil society, independent funding streams for the AI ecosystem, increased transparency and ethics requirements, greater civil society access to policy, and various procedural safeguards.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-10-16T21:06:54Z</published>
    <arxiv:comment>39 pages (14 pages main text), 3 figures, 9 tables. To be published in the Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, &amp; Society (AIES)</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <arxiv:journal_ref>Proc. AAAI/ACM Conf. AI, Ethics &amp; Soc., 7 (2024) 1539-1555</arxiv:journal_ref>
    <author>
      <name>Kevin Wei</name>
    </author>
    <author>
      <name>Carson Ezell</name>
    </author>
    <author>
      <name>Nick Gabrieli</name>
    </author>
    <author>
      <name>Chinmay Deshpande</name>
    </author>
    <arxiv:doi>10.1609/aies.v7i1.31745</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1609/aies.v7i1.31745" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2512.20902v1</id>
    <title>Embodied AI-Enhanced IoMT Edge Computing: UAV Trajectory Optimization and Task Offloading with Mobility Prediction</title>
    <updated>2025-12-24T03:06:37Z</updated>
    <link href="https://arxiv.org/abs/2512.20902v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2512.20902v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Due to their inherent flexibility and autonomous operation, unmanned aerial vehicles (UAVs) have been widely used in Internet of Medical Things (IoMT) to provide real-time biomedical edge computing service for wireless body area network (WBAN) users. In this paper, considering the time-varying task criticality characteristics of diverse WBAN users and the dual mobility between WBAN users and UAV, we investigate the dynamic task offloading and UAV flight trajectory optimization problem to minimize the weighted average task completion time of all the WBAN users, under the constraint of UAV energy consumption. To tackle the problem, an embodied AI-enhanced IoMT edge computing framework is established. Specifically, we propose a novel hierarchical multi-scale Transformer-based user trajectory prediction model based on the users' historical trajectory traces captured by the embodied AI agent (i.e., UAV). Afterwards, a prediction-enhanced deep reinforcement learning (DRL) algorithm that integrates predicted users' mobility information is designed for intelligently optimizing UAV flight trajectory and task offloading decisions. Real-word movement traces and simulation results demonstrate the superiority of the proposed methods in comparison with the existing benchmarks.</summary>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-12-24T03:06:37Z</published>
    <arxiv:primary_category term="cs.NI"/>
    <author>
      <name>Siqi Mu</name>
    </author>
    <author>
      <name>Shuo Wen</name>
    </author>
    <author>
      <name>Yang Lu</name>
    </author>
    <author>
      <name>Ruihong Jiang</name>
    </author>
    <author>
      <name>Bo Ai</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.04336v3</id>
    <title>AI-Driven Mobility Management for High-Speed Railway Communications: Compressed Measurements and Proactive Handover</title>
    <updated>2025-07-06T00:52:26Z</updated>
    <link href="https://arxiv.org/abs/2407.04336v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2407.04336v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>High-speed railway (HSR) communications are pivotal for ensuring rail safety, operations, maintenance, and delivering passenger information services. The high speed of trains creates rapidly time-varying wireless channels, increases the signaling overhead, and reduces the system throughput, making it difficult to meet the growing and stringent needs of HSR applications. In this article, we explore artificial intelligence (AI)-based beam-level and cell-level mobility management suitable for HSR communications. Particularly, we propose a compressed spatial multi-beam measurements scheme via compressive sensing for beam-level mobility management in HSR communications. In comparison to traditional down-sampling spatial beam measurements, this method leads to improved spatial-temporal beam prediction accuracy with the same measurement overhead. Moreover, we propose a novel AI-based proactive handover scheme to predict handover events and reduce radio link failure (RLF) rates in HSR communications. Compared with the traditional event A3-based handover mechanism, the proposed approach significantly reduces the RLF rates which saves 50% beam measurement overhead.</summary>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-07-05T08:23:13Z</published>
    <arxiv:primary_category term="eess.SP"/>
    <author>
      <name>Wen Li</name>
    </author>
    <author>
      <name>Wei Chen</name>
    </author>
    <author>
      <name>Shiyue Wang</name>
    </author>
    <author>
      <name>Yuanyuan Zhang</name>
    </author>
    <author>
      <name>Michail Matthaiou</name>
    </author>
    <author>
      <name>Bo Ai</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.06861v1</id>
    <title>Integrators at War: Mediating in AI-assisted Resort-to-Force Decisions</title>
    <updated>2025-01-12T16:21:33Z</updated>
    <link href="https://arxiv.org/abs/2501.06861v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2501.06861v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The integration of AI systems into the military domain is changing the way war-related decisions are made. It binds together three disparate groups of actors - developers, integrators, users - and creates a relationship between these groups and the machine, embedded in the (pre-)existing organisational and system structures. In this article, we focus on the important, but often neglected, group of integrators within such a sociotechnical system. In complex human-machine configurations, integrators carry responsibility for linking the disparate groups of developers and users in the political and military system. To act as the mediating group requires a deep understanding of the other groups' activities, perspectives and norms. We thus ask which challenges and shortcomings emerge from integrating AI systems into resort-to-force (RTF) decision-making processes, and how to address them. To answer this, we proceed in three steps. First, we conceptualise the relationship between different groups of actors and AI systems as a sociotechnical system. Second, we identify challenges within such systems for human-machine teaming in RTF decisions. We focus on challenges that arise a) from the technology itself, b) from the integrators' role in the sociotechnical system, c) from the human-machine interaction. Third, we provide policy recommendations to address these shortcomings when integrating AI systems into RTF decision-making structures.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-01-12T16:21:33Z</published>
    <arxiv:comment>32 pages. Keywords: education, artificial intelligence, AI integrators, resort to force, sociotechnical system, systems engineering</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <arxiv:journal_ref>Camb. forum AI Law gov. 1 (2025) e51</arxiv:journal_ref>
    <author>
      <name>Dennis MÃ¼ller</name>
    </author>
    <author>
      <name>Maurice Chiodo</name>
    </author>
    <author>
      <name>Mitja Sienknecht</name>
    </author>
    <arxiv:doi>10.1017/cfl.2025.10030</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1017/cfl.2025.10030" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.18527v2</id>
    <title>GOD model: Privacy Preserved AI School for Personal Assistant</title>
    <updated>2025-02-27T20:33:35Z</updated>
    <link href="https://arxiv.org/abs/2502.18527v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2502.18527v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Personal AI assistants (e.g., Apple Intelligence, Meta AI) offer proactive recommendations that simplify everyday tasks, but their reliance on sensitive user data raises concerns about privacy and trust. To address these challenges, we introduce the Guardian of Data (GOD), a secure, privacy-preserving framework for training and evaluating AI assistants directly on-device. Unlike traditional benchmarks, the GOD model measures how well assistants can anticipate user needs-such as suggesting gifts-while protecting user data and autonomy. Functioning like an AI school, it addresses the cold start problem by simulating user queries and employing a curriculum-based approach to refine the performance of each assistant. Running within a Trusted Execution Environment (TEE), it safeguards user data while applying reinforcement and imitation learning to refine AI recommendations. A token-based incentive system encourages users to share data securely, creating a data flywheel that drives continuous improvement. Specifically, users mine with their data, and the mining rate is determined by GOD's evaluation of how well their AI assistant understands them across categories such as shopping, social interactions, productivity, trading, and Web3. By integrating privacy, personalization, and trust, the GOD model provides a scalable, responsible path for advancing personal AI assistants. For community collaboration, part of the framework is open-sourced at https://github.com/PIN-AI/God-Model.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-02-24T20:30:17Z</published>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name> PIN AI Team</name>
    </author>
    <author>
      <name>Bill Sun</name>
    </author>
    <author>
      <name>Gavin Guo</name>
    </author>
    <author>
      <name>Regan Peng</name>
    </author>
    <author>
      <name>Boliang Zhang</name>
    </author>
    <author>
      <name>Shouqiao Wang</name>
    </author>
    <author>
      <name>Laura Florescu</name>
    </author>
    <author>
      <name>Xi Wang</name>
    </author>
    <author>
      <name>Davide Crapis</name>
    </author>
    <author>
      <name>Ben Wu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.14996v1</id>
    <title>Distributed Cognition for AI-supported Remote Operations: Challenges and Research Directions</title>
    <updated>2025-04-21T09:53:49Z</updated>
    <link href="https://arxiv.org/abs/2504.14996v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2504.14996v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper investigates the impact of artificial intelligence integration on remote operations, emphasising its influence on both distributed and team cognition. As remote operations increasingly rely on digital interfaces, sensors, and networked communication, AI-driven systems transform decision-making processes across domains such as air traffic control, industrial automation, and intelligent ports. However, the integration of AI introduces significant challenges, including the reconfiguration of human-AI team cognition, the need for adaptive AI memory that aligns with human distributed cognition, and the design of AI fallback operators to maintain continuity during communication disruptions. Drawing on theories of distributed and team cognition, we analyse how cognitive overload, loss of situational awareness, and impaired team coordination may arise in AI-supported environments. Based on real-world intelligent port scenarios, we propose research directions that aim to safeguard human reasoning and enhance collaborative decision-making in AI-augmented remote operations.</summary>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-04-21T09:53:49Z</published>
    <arxiv:comment>Presented at the 2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING</arxiv:comment>
    <arxiv:primary_category term="cs.HC"/>
    <arxiv:journal_ref>Proceedings of the 2025 ACM CHI Workshop on Human-AI Interaction for Augmented Reasoning</arxiv:journal_ref>
    <author>
      <name>Rune MÃ¸berg Jacobsen</name>
    </author>
    <author>
      <name>Joel Wester</name>
    </author>
    <author>
      <name>Helena BÃ¸jer DjernÃ¦s</name>
    </author>
    <author>
      <name>Niels van Berkel</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15647v2</id>
    <title>Promoting Real-Time Reflection in Synchronous Communication with Generative AI</title>
    <updated>2025-04-28T14:48:38Z</updated>
    <link href="https://arxiv.org/abs/2504.15647v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2504.15647v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Real-time reflection plays a vital role in synchronous communication. It enables users to adjust their communication strategies dynamically, thereby improving the effectiveness of their communication. Generative AI holds significant potential to enhance real-time reflection due to its ability to comprehensively understand the current context and generate personalized and nuanced content. However, it is challenging to design the way of interaction and information presentation to support the real-time workflow rather than disrupt it. In this position paper, we present a review of existing research on systems designed for reflection in different synchronous communication scenarios. Based on that, we discuss design implications on how to design human-AI interaction to support reflection in real time.</summary>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-04-22T07:12:56Z</published>
    <arxiv:comment>Presented at the 2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING</arxiv:comment>
    <arxiv:primary_category term="cs.HC"/>
    <arxiv:journal_ref>Proceedings of the 2025 ACM CHI Workshop on Human-AI Interaction for Augmented Reasoning</arxiv:journal_ref>
    <author>
      <name>Yi Wen</name>
    </author>
    <author>
      <name>Meng Xia</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.14702v2</id>
    <title>Rate-Splitting for Cell-Free Massive MIMO: Performance Analysis and Generative AI Approach</title>
    <updated>2024-09-24T04:32:08Z</updated>
    <link href="https://arxiv.org/abs/2409.14702v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2409.14702v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Cell-free (CF) massive multiple-input multipleoutput (MIMO) provides a ubiquitous coverage to user equipments (UEs) but it is also susceptible to interference. Ratesplitting (RS) effectively extracts data by decoding interference, yet its effectiveness is limited by the weakest UE. In this paper, we investigate an RS-based CF massive MIMO system, which combines strengths and mitigates weaknesses of both approaches. Considering imperfect channel state information (CSI) resulting from both pilot contamination and noise, we derive a closed-form expression for the sum spectral efficiency (SE) of the RS-based CF massive MIMO system under a spatially correlated Rician channel. Moreover, we propose low-complexity heuristic algorithms based on statistical CSI for power-splitting of common messages and power-control of private messages, and genetic algorithm is adopted as a solution for upper bound performance. Furthermore, we formulate a joint optimization problem, aiming to maximize the sum SE of the RS-based CF massive MIMO system by optimizing the power-splitting factor and power-control coefficient. Importantly, we improve a generative AI (GAI) algorithm to address this complex and nonconvexity problem by using a diffusion model to obtain solutions. Simulation results demonstrate its effectiveness and practicality in mitigating interference, especially in dynamic environments.</summary>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-09-23T04:43:01Z</published>
    <arxiv:comment>15 pages, 9 figures, Accepted in IEEE Transactions on Communications</arxiv:comment>
    <arxiv:primary_category term="cs.IT"/>
    <author>
      <name>Jiakang Zheng</name>
    </author>
    <author>
      <name>Jiayi Zhang</name>
    </author>
    <author>
      <name>Hongyang Du</name>
    </author>
    <author>
      <name>Ruichen Zhang</name>
    </author>
    <author>
      <name>Dusit Niyato</name>
    </author>
    <author>
      <name>Octavia A. Dobre</name>
    </author>
    <author>
      <name>Bo Ai</name>
    </author>
  </entry>
</feed>
