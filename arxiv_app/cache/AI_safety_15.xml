<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api//LPm9fhY3LWSEM/fM+/PakfLjhU</id>
  <title>arXiv Query: search_query=all:AI OR all:safety&amp;id_list=&amp;start=0&amp;max_results=15</title>
  <updated>2026-02-19T21:16:43Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:AI+OR+all:safety&amp;start=0&amp;max_results=15&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>15</opensearch:itemsPerPage>
  <opensearch:totalResults>74820</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2503.04743v1</id>
    <title>AI Safety is Stuck in Technical Terms -- A System Safety Response to the International AI Safety Report</title>
    <updated>2025-02-05T22:37:53Z</updated>
    <link href="https://arxiv.org/abs/2503.04743v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2503.04743v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Safety has become the central value around which dominant AI governance efforts are being shaped. Recently, this culminated in the publication of the International AI Safety Report, written by 96 experts of which 30 nominated by the Organisation for Economic Co-operation and Development (OECD), the European Union (EU), and the United Nations (UN). The report focuses on the safety risks of general-purpose AI and available technical mitigation approaches. In this response, informed by a system safety perspective, I refl ect on the key conclusions of the report, identifying fundamental issues in the currently dominant technical framing of AI safety and how this frustrates meaningful discourse and policy efforts to address safety comprehensively. The system safety discipline has dealt with the safety risks of software-based systems for many decades, and understands safety risks in AI systems as sociotechnical and requiring consideration of technical and non-technical factors and their interactions. The International AI Safety report does identify the need for system safety approaches. Lessons, concepts and methods from system safety indeed provide an important blueprint for overcoming current shortcomings in technical approaches by integrating rather than adding on non-technical factors and interventions. I conclude with why building a system safety discipline can help us overcome limitations in the European AI Act, as well as how the discipline can help shape sustainable investments into Public Interest AI.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-02-05T22:37:53Z</published>
    <arxiv:comment>A response to the International AI Safety Report, which was released in preparation for the AI Action Summit in Paris, February 2025</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <author>
      <name>Roel Dobbe</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.22183v1</id>
    <title>A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety</title>
    <updated>2025-06-27T12:45:44Z</updated>
    <link href="https://arxiv.org/abs/2506.22183v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2506.22183v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The rapid rise of open-weight and open-source foundation models is intensifying the obligation and reshaping the opportunity to make AI systems safe. This paper reports outcomes from the Columbia Convening on AI Openness and Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme involving more than forty-five researchers, engineers, and policy leaders from academia, industry, civil society, and government. Using a participatory, solutions-oriented process, the working groups produced (i) a research agenda at the intersection of safety and open source AI; (ii) a mapping of existing and needed technical interventions and open source tools to safely and responsibly deploy open foundation models across the AI development workflow; and (iii) a mapping of the content safety filter ecosystem with a proposed roadmap for future research and development. We find that openness -- understood as transparent weights, interoperable tooling, and public governance -- can enhance safety by enabling independent scrutiny, decentralized mitigation, and culturally plural oversight. However, significant gaps persist: scarce multimodal and multilingual benchmarks, limited defenses against prompt-injection and compositional attacks in agentic systems, and insufficient participatory mechanisms for communities most affected by AI harms. The paper concludes with a roadmap of five priority research directions, emphasizing participatory inputs, future-proof content filters, ecosystem-wide safety infrastructure, rigorous agentic safeguards, and expanded harm taxonomies. These recommendations informed the February 2025 French AI Action Summit and lay groundwork for an open, plural, and accountable AI safety discipline.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-06-27T12:45:44Z</published>
    <arxiv:comment>Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Camille François</name>
    </author>
    <author>
      <name>Ludovic Péran</name>
    </author>
    <author>
      <name>Ayah Bdeir</name>
    </author>
    <author>
      <name>Nouha Dziri</name>
    </author>
    <author>
      <name>Will Hawkins</name>
    </author>
    <author>
      <name>Yacine Jernite</name>
    </author>
    <author>
      <name>Sayash Kapoor</name>
    </author>
    <author>
      <name>Juliet Shen</name>
    </author>
    <author>
      <name>Heidy Khlaaf</name>
    </author>
    <author>
      <name>Kevin Klyman</name>
    </author>
    <author>
      <name>Nik Marda</name>
    </author>
    <author>
      <name>Marie Pellat</name>
    </author>
    <author>
      <name>Deb Raji</name>
    </author>
    <author>
      <name>Divya Siddarth</name>
    </author>
    <author>
      <name>Aviya Skowron</name>
    </author>
    <author>
      <name>Joseph Spisak</name>
    </author>
    <author>
      <name>Madhulika Srikumar</name>
    </author>
    <author>
      <name>Victor Storchan</name>
    </author>
    <author>
      <name>Audrey Tang</name>
    </author>
    <author>
      <name>Jen Weedon</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2601.16513v1</id>
    <title>Competing Visions of Ethical AI: A Case Study of OpenAI</title>
    <updated>2026-01-23T07:26:45Z</updated>
    <link href="https://arxiv.org/abs/2601.16513v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2601.16513v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Introduction. AI Ethics is framed distinctly across actors and stakeholder groups. We report results from a case study of OpenAI analysing ethical AI discourse. Method. Research addressed: How has OpenAI's public discourse leveraged 'ethics', 'safety', 'alignment' and adjacent related concepts over time, and what does discourse signal about framing in practice? A structured corpus, differentiating between communication for a general audience and communication with an academic audience, was assembled from public documentation. Analysis. Qualitative content analysis of ethical themes combined inductively derived and deductively applied codes. Quantitative analysis leveraged computational content analysis methods via NLP to model topics and quantify changes in rhetoric over time. Visualizations report aggregate results. For reproducible results, we have released our code at https://github.com/famous-blue-raincoat/AI_Ethics_Discourse. Results. Results indicate that safety and risk discourse dominate OpenAI's public communication and documentation, without applying academic and advocacy ethics frameworks or vocabularies. Conclusions. Implications for governance are presented, along with discussion of ethics-washing practices in industry.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2026-01-23T07:26:45Z</published>
    <arxiv:comment>iConference 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <author>
      <name>Melissa Wilfley</name>
    </author>
    <author>
      <name>Mengting Ai</name>
    </author>
    <author>
      <name>Madelyn Rose Sanfilippo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.02842v1</id>
    <title>Foundations of GenIR</title>
    <updated>2025-01-06T08:38:29Z</updated>
    <link href="https://arxiv.org/abs/2501.02842v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2501.02842v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The chapter discusses the foundational impact of modern generative AI models on information access (IA) systems. In contrast to traditional AI, the large-scale training and superior data modeling of generative AI models enable them to produce high-quality, human-like responses, which brings brand new opportunities for the development of IA paradigms. In this chapter, we identify and introduce two of them in details, i.e., information generation and information synthesis. Information generation allows AI to create tailored content addressing user needs directly, enhancing user experience with immediate, relevant outputs. Information synthesis leverages the ability of generative AI to integrate and reorganize existing information, providing grounded responses and mitigating issues like model hallucination, which is particularly valuable in scenarios requiring precision and external knowledge. This chapter delves into the foundational aspects of generative models, including architecture, scaling, and training, and discusses their applications in multi-modal scenarios. Additionally, it examines the retrieval-augmented generation paradigm and other methods for corpus modeling and understanding, demonstrating how generative AI can enhance information access systems. It also summarizes potential challenges and fruitful directions for future studies.</summary>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-01-06T08:38:29Z</published>
    <arxiv:comment>Chapter 2 of the book on Information Access in the Era of Generative AI</arxiv:comment>
    <arxiv:primary_category term="cs.IR"/>
    <author>
      <name>Qingyao Ai</name>
    </author>
    <author>
      <name>Jingtao Zhan</name>
    </author>
    <author>
      <name>Yiqun Liu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.10217v3</id>
    <title>Data-Centric Safety and Ethical Measures for Data and AI Governance</title>
    <updated>2025-07-01T01:49:13Z</updated>
    <link href="https://arxiv.org/abs/2506.10217v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2506.10217v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Datasets play a key role in imparting advanced capabilities to artificial intelligence (AI) foundation models that can be adapted to various downstream tasks. These downstream applications can introduce both beneficial and harmful capabilities -- resulting in dual use AI foundation models, with various technical and regulatory approaches to monitor and manage these risks. However, despite the crucial role of datasets, responsible dataset design and ensuring data-centric safety and ethical practices have received less attention. In this study, we pro-pose responsible dataset design framework that encompasses various stages in the AI and dataset lifecycle to enhance safety measures and reduce the risk of AI misuse due to low quality, unsafe and unethical data content. This framework is domain agnostic, suitable for adoption for various applications and can promote responsible practices in dataset creation, use, and sharing to facilitate red teaming, minimize risks, and increase trust in AI models.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-06-11T22:26:50Z</published>
    <arxiv:comment>Paper accepted and presented at the AAAI 2025 Workshop on Datasets and Evaluators of AI Safety https://sites.google.com/view/datasafe25/home</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <author>
      <name>Srija Chakraborty</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.05282v2</id>
    <title>International Scientific Report on the Safety of Advanced AI (Interim Report)</title>
    <updated>2025-04-09T11:34:12Z</updated>
    <link href="https://arxiv.org/abs/2412.05282v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2412.05282v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>This is the interim publication of the first International Scientific Report on the Safety of Advanced AI. The report synthesises the scientific understanding of general-purpose AI -- AI that can perform a wide variety of tasks -- with a focus on understanding and managing its risks. A diverse group of 75 AI experts contributed to this report, including an international Expert Advisory Panel nominated by 30 countries, the EU, and the UN. Led by the Chair, these independent experts collectively had full discretion over the report's content.
  The final report is available at arXiv:2501.17805</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-11-05T15:47:23Z</published>
    <arxiv:comment>Available under the open government license at https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <author>
      <name>Sören Mindermann</name>
    </author>
    <author>
      <name>Daniel Privitera</name>
    </author>
    <author>
      <name>Tamay Besiroglu</name>
    </author>
    <author>
      <name>Rishi Bommasani</name>
    </author>
    <author>
      <name>Stephen Casper</name>
    </author>
    <author>
      <name>Yejin Choi</name>
    </author>
    <author>
      <name>Danielle Goldfarb</name>
    </author>
    <author>
      <name>Hoda Heidari</name>
    </author>
    <author>
      <name>Leila Khalatbari</name>
    </author>
    <author>
      <name>Shayne Longpre</name>
    </author>
    <author>
      <name>Vasilios Mavroudis</name>
    </author>
    <author>
      <name>Mantas Mazeika</name>
    </author>
    <author>
      <name>Kwan Yee Ng</name>
    </author>
    <author>
      <name>Chinasa T. Okolo</name>
    </author>
    <author>
      <name>Deborah Raji</name>
    </author>
    <author>
      <name>Theodora Skeadas</name>
    </author>
    <author>
      <name>Florian Tramèr</name>
    </author>
    <author>
      <name>Bayo Adekanmbi</name>
    </author>
    <author>
      <name>Paul Christiano</name>
    </author>
    <author>
      <name>David Dalrymple</name>
    </author>
    <author>
      <name>Thomas G. Dietterich</name>
    </author>
    <author>
      <name>Edward Felten</name>
    </author>
    <author>
      <name>Pascale Fung</name>
    </author>
    <author>
      <name>Pierre-Olivier Gourinchas</name>
    </author>
    <author>
      <name>Nick Jennings</name>
    </author>
    <author>
      <name>Andreas Krause</name>
    </author>
    <author>
      <name>Percy Liang</name>
    </author>
    <author>
      <name>Teresa Ludermir</name>
    </author>
    <author>
      <name>Vidushi Marda</name>
    </author>
    <author>
      <name>Helen Margetts</name>
    </author>
    <author>
      <name>John A. McDermid</name>
    </author>
    <author>
      <name>Arvind Narayanan</name>
    </author>
    <author>
      <name>Alondra Nelson</name>
    </author>
    <author>
      <name>Alice Oh</name>
    </author>
    <author>
      <name>Gopal Ramchurn</name>
    </author>
    <author>
      <name>Stuart Russell</name>
    </author>
    <author>
      <name>Marietje Schaake</name>
    </author>
    <author>
      <name>Dawn Song</name>
    </author>
    <author>
      <name>Alvaro Soto</name>
    </author>
    <author>
      <name>Lee Tiedrich</name>
    </author>
    <author>
      <name>Gaël Varoquaux</name>
    </author>
    <author>
      <name>Andrew Yao</name>
    </author>
    <author>
      <name>Ya-Qin Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.17419v1</id>
    <title>AI Safety: Necessary, but insufficient and possibly problematic</title>
    <updated>2024-03-26T06:18:42Z</updated>
    <link href="https://arxiv.org/abs/2403.17419v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2403.17419v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This article critically examines the recent hype around AI safety. We first start with noting the nature of the AI safety hype as being dominated by governments and corporations, and contrast it with other avenues within AI research on advancing social good. We consider what 'AI safety' actually means, and outline the dominant concepts that the digital footprint of AI safety aligns with. We posit that AI safety has a nuanced and uneasy relationship with transparency and other allied notions associated with societal good, indicating that it is an insufficient notion if the goal is that of societal good in a broad sense. We note that the AI safety debate has already influenced some regulatory efforts in AI, perhaps in not so desirable directions. We also share our concerns on how AI safety may normalize AI that advances structural harm through providing exploitative and harmful AI with a veneer of safety.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-03-26T06:18:42Z</published>
    <arxiv:comment>AI &amp; Soc (2024)</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Deepak P</name>
    </author>
    <arxiv:doi>10.1007/s00146-024-01899-y</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1007/s00146-024-01899-y" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.12400v1</id>
    <title>Towards The Ultimate Brain: Exploring Scientific Discovery with ChatGPT AI</title>
    <updated>2023-07-08T09:59:22Z</updated>
    <link href="https://arxiv.org/abs/2308.12400v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2308.12400v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper presents a novel approach to scientific discovery using an artificial intelligence (AI) environment known as ChatGPT, developed by OpenAI. This is the first paper entirely generated with outputs from ChatGPT. We demonstrate how ChatGPT can be instructed through a gamification environment to define and benchmark hypothetical physical theories. Through this environment, ChatGPT successfully simulates the creation of a new improved model, called GPT$^4$, which combines the concepts of GPT in AI (generative pretrained transformer) and GPT in physics (generalized probabilistic theory). We show that GPT$^4$ can use its built-in mathematical and statistical capabilities to simulate and analyze physical laws and phenomena. As a demonstration of its language capabilities, GPT$^4$ also generates a limerick about itself. Overall, our results demonstrate the promising potential for human-AI collaboration in scientific discovery, as well as the importance of designing systems that effectively integrate AI's capabilities with human intelligence.</summary>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-07-08T09:59:22Z</published>
    <arxiv:comment>26 pages. To appear in AI Magazine</arxiv:comment>
    <arxiv:primary_category term="cs.OH"/>
    <arxiv:journal_ref>AI Magazine 44, 328 (2023)</arxiv:journal_ref>
    <author>
      <name>Gerardo Adesso</name>
    </author>
    <arxiv:doi>10.1002/aaai.12113</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1002/aaai.12113" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.4924v1</id>
    <title>Laser pointer prohibition: improving safety or driving misclassification</title>
    <updated>2014-06-19T00:46:15Z</updated>
    <link href="https://arxiv.org/abs/1406.4924v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1406.4924v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>It is well known that since 2008 Australia has had some of the world's most restrictive laws regarding the possession and importation of "laser pointers" with powers exceeding 1 mW. Now four years on Australia is used as a test case and question whether this has actually improved safety for those wishing to purchase these devices or if it has impacted on the availability of prohibited devices. Results from the analysis of over 40 laser pointers legitimately purchased in Australia from local and International suppliers are presented. Specifically lasers that are readily available to everyday consumers through the simple on-line search "laser pointer 1mw" are targeted. The parameters investigated are quoted power versus measured power, correct representation in advertising and adherence to laser standards as related to specified use and purchase price. The analysis indicates that the suppliers in this market have learnt how to bypass the prohibition and the impact on general safety in these cases is detrimental.</summary>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <published>2014-06-19T00:46:15Z</published>
    <arxiv:comment>Presented at the International Laser Safety Conference (ILSC), Orlando, Florida, March 2013</arxiv:comment>
    <arxiv:primary_category term="physics.soc-ph"/>
    <arxiv:journal_ref>Proceedings of the International Laser Safety Conference, 2013, pp. 48-54</arxiv:journal_ref>
    <author>
      <name>Trevor A Wheatley</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01298v2</id>
    <title>Meaningful human control: actionable properties for AI system development</title>
    <updated>2022-05-19T15:28:01Z</updated>
    <link href="https://arxiv.org/abs/2112.01298v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2112.01298v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>How can humans remain in control of artificial intelligence (AI)-based systems designed to perform tasks autonomously? Such systems are increasingly ubiquitous, creating benefits - but also undesirable situations where moral responsibility for their actions cannot be properly attributed to any particular person or group. The concept of meaningful human control has been proposed to address responsibility gaps and mitigate them by establishing conditions that enable a proper attribution of responsibility for humans; however, clear requirements for researchers, designers, and engineers are yet inexistent, making the development of AI-based systems that remain under meaningful human control challenging. In this paper, we address the gap between philosophical theory and engineering practice by identifying, through an iterative process of abductive thinking, four actionable properties for AI-based systems under meaningful human control, which we discuss making use of two applications scenarios: automated vehicles and AI-based hiring. First, a system in which humans and AI algorithms interact should have an explicitly defined domain of morally loaded situations within which the system ought to operate. Second, humans and AI agents within the system should have appropriate and mutually compatible representations. Third, responsibility attributed to a human should be commensurate with that human's ability and authority to control the system. Fourth, there should be explicit links between the actions of the AI agents and actions of humans who are aware of their moral responsibility. We argue that these four properties will support practically-minded professionals to take concrete steps toward designing and engineering for AI systems that facilitate meaningful human control.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2021-11-25T11:05:37Z</published>
    <arxiv:comment>Preprint. Published AI and Ethics (2022): https://doi.org/10.1007/s43681-022-00167-3</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <arxiv:journal_ref>AI Ethics (2022)</arxiv:journal_ref>
    <author>
      <name>Luciano Cavalcante Siebert</name>
    </author>
    <author>
      <name>Maria Luce Lupetti</name>
    </author>
    <author>
      <name>Evgeni Aizenberg</name>
    </author>
    <author>
      <name>Niek Beckers</name>
    </author>
    <author>
      <name>Arkady Zgonnikov</name>
    </author>
    <author>
      <name>Herman Veluwenkamp</name>
    </author>
    <author>
      <name>David Abbink</name>
    </author>
    <author>
      <name>Elisa Giaccardi</name>
    </author>
    <author>
      <name>Geert-Jan Houben</name>
    </author>
    <author>
      <name>Catholijn M. Jonker</name>
    </author>
    <author>
      <name>Jeroen van den Hoven</name>
    </author>
    <author>
      <name>Deborah Forster</name>
    </author>
    <author>
      <name>Reginald L. Lagendijk</name>
    </author>
    <arxiv:doi>10.1007/s43681-022-00167-3</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1007/s43681-022-00167-3" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.00025v3</id>
    <title>Need of AI in Modern Education: in the Eyes of Explainable AI (xAI)</title>
    <updated>2025-01-02T21:59:22Z</updated>
    <link href="https://arxiv.org/abs/2408.00025v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2408.00025v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Modern Education is not \textit{Modern} without AI. However, AI's complex nature makes understanding and fixing problems challenging. Research worldwide shows that a parent's income greatly influences a child's education. This led us to explore how AI, especially complex models, makes important decisions using Explainable AI tools. Our research uncovered many complexities linked to parental income and offered reasonable explanations for these decisions. However, we also found biases in AI that go against what we want from AI in education: clear transparency and equal access for everyone. These biases can impact families and children's schooling, highlighting the need for better AI solutions that offer fair opportunities to all. This chapter tries to shed light on the complex ways AI operates, especially concerning biases. These are the foundational steps towards better educational policies, which include using AI in ways that are more reliable, accountable, and beneficial for everyone involved.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-07-31T08:11:33Z</published>
    <arxiv:comment>Chapter in the book: Blockchain and AI in Shaping the Modern Education System, CRC Press, Taylor &amp; Francis Group, USA</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <arxiv:journal_ref>Blockchain and AI in Shaping the Modern Education System 2025</arxiv:journal_ref>
    <author>
      <name>Supriya Manna</name>
    </author>
    <author>
      <name>Niladri Sett</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.15284v6</id>
    <title>Beyond principlism: Practical strategies for ethical AI use in research practices</title>
    <updated>2025-06-20T02:59:45Z</updated>
    <link href="https://arxiv.org/abs/2401.15284v6" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2401.15284v6" rel="related" type="application/pdf" title="pdf"/>
    <summary>The rapid adoption of generative artificial intelligence (AI) in scientific research, particularly large language models (LLMs), has outpaced the development of ethical guidelines, leading to a "Triple-Too" problem: too many high-level ethical initiatives, too abstract principles lacking contextual and practical relevance, and too much focus on restrictions and risks over benefits and utilities. Existing approaches--principlism (reliance on abstract ethical principles), formalism (rigid application of rules), and technological solutionism (overemphasis on technological fixes)--offer little practical guidance for addressing ethical challenges of AI in scientific research practices. To bridge the gap between abstract principles and day-to-day research practices, a user-centered, realism-inspired approach is proposed here. It outlines five specific goals for ethical AI use: 1) understanding model training and output, including bias mitigation strategies; 2) respecting privacy, confidentiality, and copyright; 3) avoiding plagiarism and policy violations; 4) applying AI beneficially compared to alternatives; and 5) using AI transparently and reproducibly. Each goal is accompanied by actionable strategies and realistic cases of misuse and corrective measures. I argue that ethical AI application requires evaluating its utility against existing alternatives rather than isolated performance metrics. Additionally, I propose documentation guidelines to enhance transparency and reproducibility in AI-assisted research. Moving forward, we need targeted professional development, training programs, and balanced enforcement mechanisms to promote responsible AI use while fostering innovation. By refining these ethical guidelines and adapting them to emerging AI capabilities, we can accelerate scientific progress without compromising research integrity.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-01-27T03:53:25Z</published>
    <arxiv:comment>Published in: AI and Ethics, 2025</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <arxiv:journal_ref>AI and Ethics, 5, 2719-2731 (2025)</arxiv:journal_ref>
    <author>
      <name>Zhicheng Lin</name>
    </author>
    <arxiv:doi>10.1007/s43681-024-00585-5</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1007/s43681-024-00585-5" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.16562v1</id>
    <title>Vernacularizing Taxonomies of Harm is Essential for Operationalizing Holistic AI Safety</title>
    <updated>2024-10-21T22:47:48Z</updated>
    <link href="https://arxiv.org/abs/2410.16562v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2410.16562v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Operationalizing AI ethics and safety principles and frameworks is essential to realizing the potential benefits and mitigating potential harms caused by AI systems. To that end, actors across industry, academia, and regulatory bodies have created formal taxonomies of harm to support operationalization efforts. These include novel holistic methods that go beyond exclusive reliance on technical benchmarking. However, our paper argues that such taxonomies must also be transferred into local categories to be readily implemented in sector-specific AI safety operationalization efforts, and especially in underresourced or high-risk sectors. This is because many sectors are constituted by discourses, norms, and values that "refract" or even directly conflict with those operating in society more broadly. Drawing from emerging anthropological theories of human rights, we propose that the process of "vernacularization"--a participatory, decolonial practice distinct from doctrinary "translation" (the dominant mode of AI safety operationalization)--can help bridge this gap. To demonstrate this point, we consider the education sector, and identify precisely how vernacularizing a leading holistic taxonomy of harm leads to a clearer view of how harms AI systems may cause are substantially intensified when deployed in educational spaces. We conclude by discussing the generalizability of vernacularization as a useful AI safety methodology.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-10-21T22:47:48Z</published>
    <arxiv:comment>Accepted to the Proceedings of the Conference on AI Ethics and Society (AIES), 2024</arxiv:comment>
    <arxiv:primary_category term="cs.CY"/>
    <author>
      <name>Wm. Matthew Kennedy</name>
    </author>
    <author>
      <name>Daniel Vargas Campos</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.16770v1</id>
    <title>DeBiasMe: De-biasing Human-AI Interactions with Metacognitive AIED (AI in Education) Interventions</title>
    <updated>2025-04-23T14:41:31Z</updated>
    <link href="https://arxiv.org/abs/2504.16770v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2504.16770v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>While generative artificial intelligence (Gen AI) increasingly transforms academic environments, a critical gap exists in understanding and mitigating human biases in AI interactions, such as anchoring and confirmation bias. This position paper advocates for metacognitive AI literacy interventions to help university students critically engage with AI and address biases across the Human-AI interaction workflows. The paper presents the importance of considering (1) metacognitive support with deliberate friction focusing on human bias; (2) bi-directional Human-AI interaction intervention addressing both input formulation and output interpretation; and (3) adaptive scaffolding that responds to diverse user engagement patterns. These frameworks are illustrated through ongoing work on "DeBiasMe," AIED (AI in Education) interventions designed to enhance awareness of cognitive biases while empowering user agency in AI interactions. The paper invites multiple stakeholders to engage in discussions on design and evaluation methods for scaffolding mechanisms, bias visualization, and analysis frameworks. This position contributes to the emerging field of AI-augmented learning by emphasizing the critical role of metacognition in helping students navigate the complex interaction between human, statistical, and systemic biases in AI use while highlighting how cognitive adaptation to AI systems must be explicitly integrated into comprehensive AI literacy frameworks.</summary>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-04-23T14:41:31Z</published>
    <arxiv:comment>Presented at the 2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING</arxiv:comment>
    <arxiv:primary_category term="cs.HC"/>
    <arxiv:journal_ref>Proceedings of the 2025 ACM CHI Workshop on Human-AI Interaction for Augmented Reasoning</arxiv:journal_ref>
    <author>
      <name>Chaeyeon Lim</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.05388v3</id>
    <title>An AI System Evaluation Framework for Advancing AI Safety: Terminology, Taxonomy, Lifecycle Mapping</title>
    <updated>2024-05-15T06:19:04Z</updated>
    <link href="https://arxiv.org/abs/2404.05388v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2404.05388v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>The advent of advanced AI underscores the urgent need for comprehensive safety evaluations, necessitating collaboration across communities (i.e., AI, software engineering, and governance). However, divergent practices and terminologies across these communities, combined with the complexity of AI systems-of which models are only a part-and environmental affordances (e.g., access to tools), obstruct effective communication and comprehensive evaluation. This paper proposes a framework for AI system evaluation comprising three components: 1) harmonised terminology to facilitate communication across communities involved in AI safety evaluation; 2) a taxonomy identifying essential elements for AI system evaluation; 3) a mapping between AI lifecycle, stakeholders, and requisite evaluations for accountable AI supply chain. This framework catalyses a deeper discourse on AI system evaluation beyond model-centric approaches.</summary>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-04-08T10:49:59Z</published>
    <arxiv:comment>1st ACM International Conference on AI-powered Software (AIware)</arxiv:comment>
    <arxiv:primary_category term="cs.SE"/>
    <author>
      <name>Boming Xia</name>
    </author>
    <author>
      <name>Qinghua Lu</name>
    </author>
    <author>
      <name>Liming Zhu</name>
    </author>
    <author>
      <name>Zhenchang Xing</name>
    </author>
    <arxiv:doi>10.1145/3664646.3664766</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1145/3664646.3664766" title="doi"/>
  </entry>
</feed>
